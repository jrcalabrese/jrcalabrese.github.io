---
title: "Repeated Measures Cheatsheet"
output: 
  distill::distill_article:
    toc: true
---

```{r setup, echo=TRUE, warning = FALSE, message = FALSE}
library(tidyverse)
library(sjPlot)
library(lme4)
data(mtcars)
```

# Symbols

## Basic

| Symbol             | Mean        | Symbol             | Meaning                        |     |
| :----------------- | :---------- | :----------------- | :----------------------------- | :-- |
| $$ \beta_{0} $$    | Intercept   | $$ \sigma $$       | Variance                       |     |
| $$ \beta_{1} $$    | Slope       | $$ \sigma^2 $$     | Standard deviation             |     |
| $$ \epsilon_{i} $$ | Error       | $$ x_{1} $$        | Sample statistic               |     |
| $$ \hat{y} $$      | Estimate    | $$ x'_{1} $$       | Mean-centered sample statistic |     |
| $$ \bar{y} $$      | Average     | $$ \mu $$          | Population parameter           |     |
| $$ N $$            | Sample size | $$ N \sim (0,1) $$ | Population distribution        |     |           

# Regression

Independent variables: $x_1$, $x_2$, $x_3$, ...

Dependent variable: $Y$

## Intercept only

```{r results = "asis"}
m1 <- lm(mpg ~ 1, mtcars)
tab_model(m1, 
          show.ci = FALSE, show.se = TRUE, show.stat = TRUE,
          string.intercept = "Intercept", string.se = "SE", string.stat = "t",
          dv.labels = "Intercept Only")
```

$$
\operatorname{Y_{i}} = \beta_{0} + \epsilon_{i}
$$

## Simple linear regression

```{r}
m2 <- lm(mpg ~ hp, mtcars)
tab_model(m2,           
          show.ci = FALSE, show.se = TRUE, show.stat = TRUE,
          string.intercept = "Intercept", string.se = "SE", string.stat = "t",
          dv.labels = "Simple linear regression")
```

$$
\operatorname{Y_{i}} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}
$$

## Simple linear regresion with mean-centered x

```{r}
mtcars$hp_c <- scale(mtcars$hp, center=TRUE, scale=FALSE) 
m3 <- lm(mpg ~ hp_c, mtcars)
tab_model(m3,           
          show.ci = FALSE, show.se = TRUE, show.stat = TRUE,
          string.intercept = "Intercept", string.se = "SE", string.stat = "t",
          dv.labels = "Simple linear regression with mean-centered x")
```

$$
\operatorname{Y_{i}} = \beta_{0} + \beta_{1}x'_{i} + \epsilon_{i}
$$

## Multiple linear regression

```{r}
m4 <- lm(mpg ~ hp + qsec, mtcars)
tab_model(m4, 
          show.ci = FALSE, show.se = TRUE, show.stat = TRUE,
          string.intercept = "Intercept", string.se = "SE", string.stat = "t",
          dv.labels = "Multiple linear regression")
```

$$
\operatorname{Y_{i}} = \beta_{0} + \beta_{1}x_1{i} + \beta_{2}x_2{i} + \epsilon_{i}
$$

## Interaction / Moderation

The effect of one predictor depends on the level of the other predictor, i.e., the effect of one predictor is *moderated* by the level of the other predictor.

$x_1$ is the focal predictor and $x_2$ is the moderating variable.

```{r}
m5 <- lm(mpg ~ hp + qsec + hp:qsec, mtcars)
summary(m5)
tab_model(m5, 
          show.ci = FALSE, show.se = TRUE, show.stat = TRUE,
          string.intercept = "Intercept", string.se = "SE", string.stat = "t", 
          dv.labels = "Moderated regression")
```

$$
\hat{\operatorname{Y}} = \hat{\beta_{0}} + \hat{\beta}_{1}x_1 + \hat{\beta}_{2}x_2 + \hat{\beta}_{3}x_1x_2
$$

Also can be written as:

$$
\hat{\operatorname{Y}} = \hat{\beta_{0}} + \hat{\beta}_{1}x_1 + (\hat{\beta}_{2} + \hat{\beta}_{3}x_1)x_2 + \epsilon_{i}
$$

# Nested data

| Symbol          | Meaning                                                                                     |
| :-------------- | :------------------------------------------------------------------------------------------ |
| $i$             | Individual or case (Level-1)                                                                |
| $j$             | Group or cluster (where $j$ = 1, ..., $J$; Level-2)                                         |
| $\gamma_00$     | Fixed intercept and the grand mean, i.e., the overall average value of $\bar{y}$            |
| $\upsilon_{0j}$ | Between-group variability: the deviation of group $j$ from the overall average $\gamma_00$. |
| $\epsilon_{ij}$ | Within-group variability: the case residual (within-group).                                 |
| $\tau_{00}$     | Variance of the level-2 random intercepts                                                   |
| $\sigma^2$      | Variance of the level-1 random intercepts                                                   |

## Fixed effects model

```{r}
# m6 <- ???
# idk how to do this in R...
```

$$ 
\operatorname{Y_{i}} = {\beta}_{1}x_{1i} + {\beta}_{2}x_{2i} + ... + {\beta}_{p}x_{pi} + \epsilon_{i}
$$
$$
\epsilon_{i} \sim_{iid} N(0, \sigma^2)
$$

## Random effects ANOVA

Also known as a one-way random effects model.

The Level-1 equation is a model for within-group differences, whereas the Level-2 equation is a model for between-group differences.

The *fixed effects* of the model are constant for all cases in the population and do not carry $i$ or $j$ subscripts. The Greek symbol gamma ($\gamma$) denotes these effects.

The *random effects* of the model vary across Level-1 and Level-2 units. Effects that vary across Level-2 units are denoted by $u$; effects that vary across Level-1 units are denoted by $\epsilon$.

```{r}
data(sleepstudy)
m7 <- lme4::lmer(Reaction ~ 1 + (1|Subject), sleepstudy)
summary(m7)
tab_model(m7,           
          show.ci = FALSE, show.se = TRUE, show.stat = TRUE,
          string.intercept = "Intercept", string.se = "SE", string.stat = "t", 
          dv.labels = "Random effects ANOVA")
```

Level-1:
$$ 
\operatorname{Y_{ij}} = \beta_{0j} + \epsilon_{ij}
$$
$$
\epsilon_{i} \sim N(0, \sigma^2)
$$

Level-2:
$$ 
\operatorname{\beta_{0j}} = \gamma_{00} + \upsilon_{0j}
$$
$$
\upsilon_{0j} \sim_{iid} N(0, \tau_{00})
$$

Reduced:
$$ 
\operatorname{Y_{ij}} = \gamma_{00} + \upsilon_{0j} + \epsilon_{ij}
$$

## Random intercept regression model

```{r}
#m8 <- ??
```

$x_{ij}$ is a Level-1 predictor because it varies between group $j$ and case $i$. This model has four parameters:

* A fixed intercept, $\gamma_{00}$
* A fixed effect of $x_{ij}$, $\gamma_{10}$
* Residual variance of the Level-2 random intercepts, $\tau_{00}$
* Residual variance at Level-1, $\sigma^2$

The fixed effects are $\gamma_{00}$ and $\gamma_{10}$. The random effect is $\tau_{00}$. The covariance parameters are $\tau_{00}$ and $\sigma^2$. 

Level-1:
$$ 
\operatorname{Y_{ij}} = \beta_{0j} + \beta_{1j}x_{ij} + \epsilon_{ij}
$$
$$
\epsilon_{i} \sim N(0, \sigma^2)
$$

Level-2:
$$ 
\operatorname{\beta_{0j}} = \gamma_{00} + \upsilon_{0j}
$$
$$
\operatorname{\beta_{1j}} = \gamma_{10}
$$
$$
\upsilon_{0j} \sim_{iid} N(0, \tau_{00})
$$

Reduced:
$$ 
\operatorname{Y_{ij}} = \gamma_{00} + \gamma_{10}x_{ij} + \upsilon_{0j} + \epsilon_{ij}
$$

## Means as outcomes

Here, we incorporate Level-2 predictors to the random effects ANOVA model.

In a means as outcomes model, the means $\beta_{0j}$ are predicted with at least one Level-2 variable, $w$. Level-2 variables are literally *cluster means*, and the model
regresses $\beta_{0j}$ onto these means.

The fixed effects are $\gamma_{00}$ and $\gamma_{01}$. The random effect is $\tau_{00}$. The covariance parameters are $\tau_{00}$ and $\sigma^2$. 

Level-1:
$$ 
\operatorname{Y_{ij}} = \beta_{0j} + \epsilon_{ij}
$$
$$
\epsilon_{i} \sim N(0, \sigma^2)
$$

Level-2:
$$ 
\operatorname{\beta_{0j}} = \gamma_{00} + \gamma_{01}w_{j} + \upsilon_{0j}
$$
$$
\upsilon_{0j} \sim_{iid} N(0, \tau_{00})
$$

Reduced:
$$ 
\operatorname{Y_{ij}} = \gamma_{00} + \gamma_{01}w_{j} + \upsilon_{0j} + \epsilon_{ij}
$$

## Intercepts as outcomes

Here, we incorporate Level-2 predictors to the random intercepts regression model. The random intercepts $\beta_{0j}$ are predictor with at least one Level-2 variable $w$.

The fixed effects are $\gamma_{00}$ and $\gamma_{10}$. The random effect is $\tau_{00}$. The covariance parameters are $\tau_{00}$ and $\sigma^2$. 

Level-1:
$$ 
\operatorname{Y_{ij}} = \beta_{0j} + \beta_{1j}x_{ij} + \epsilon_{ij}
$$
$$
\epsilon_{i} \sim N(0, \sigma^2)
$$

Level-2:
$$ 
\operatorname{\beta_{0j}} = \gamma_{00} + \gamma_{01}w_{j} + \upsilon_{0j}
$$
$$
\operatorname{\beta_{1j}} = \gamma_{10}
$$
$$
\upsilon_{0j} \sim_{iid} N(0, \tau_{00})
$$

Reduced:
$$ 
\operatorname{Y_{ij}} = \gamma_{00} + + \gamma_{01}w_{j} + \gamma_{10}x_{ij} + \upsilon_{0j} + \epsilon_{ij}
$$
